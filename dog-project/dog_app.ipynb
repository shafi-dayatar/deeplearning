{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-723e6c610f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# load color (BGR) image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# convert BGR image to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "human_detection_per = 0, dog_detection_per = 0\n",
    "for file in human_files:\n",
    "    if face_detector(file):\n",
    "        human_detection_per += 1\n",
    "        \n",
    "for file in human_files:\n",
    "    if face_detector(file):\n",
    "        dog_detection_per += 1\n",
    "\n",
    "\n",
    "print(\"Human face detected %d \" % human_detection_per)\n",
    "print(\"Human face detected in dog files %d \" % dog_detection_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x117c56390>\n",
      "<keras.layers.convolutional.Conv2D object at 0x117c24650>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x117c24750>\n",
      "<keras.layers.core.Activation object at 0x117cb6610>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x117cc6f10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x117e19d10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x117c56550>\n",
      "<keras.layers.core.Activation object at 0x117e2dfd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x117eba8d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x117e69ad0>\n",
      "<keras.layers.core.Activation object at 0x118350410>\n",
      "<keras.layers.convolutional.Conv2D object at 0x1183cc710>\n",
      "<keras.layers.convolutional.Conv2D object at 0x118447410>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x118380cd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x1184d2e10>\n",
      "<keras.layers.merge.Add object at 0x118541f90>\n",
      "<keras.layers.core.Activation object at 0x117c24590>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11987fe90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x119831e10>\n",
      "<keras.layers.core.Activation object at 0x119946d10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x119967ed0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x119967390>\n",
      "<keras.layers.core.Activation object at 0x1199aaed0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11a709e90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11a6b7490>\n",
      "<keras.layers.merge.Add object at 0x11ac4ffd0>\n",
      "<keras.layers.core.Activation object at 0x11acc0e50>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11ac939d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11a7ef050>\n",
      "<keras.layers.core.Activation object at 0x11ad9d450>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11ae2bd10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11adc9910>\n",
      "<keras.layers.core.Activation object at 0x11af013d0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11af81d10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11af33910>\n",
      "<keras.layers.merge.Add object at 0x11b09c590>\n",
      "<keras.layers.core.Activation object at 0x117fac690>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11c6c3ad0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x1164d6490>\n",
      "<keras.layers.core.Activation object at 0x11c989650>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11ca92650>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11cba9e50>\n",
      "<keras.layers.core.Activation object at 0x11b2acd10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11b327b90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11b4e5810>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11b2eef50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11b4fa7d0>\n",
      "<keras.layers.merge.Add object at 0x11b60bf50>\n",
      "<keras.layers.core.Activation object at 0x11b663410>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11b66ef90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11b63e810>\n",
      "<keras.layers.core.Activation object at 0x11b75fc90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11b8047d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11b7a42d0>\n",
      "<keras.layers.core.Activation object at 0x11b7f2fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11baec990>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11ba5c610>\n",
      "<keras.layers.merge.Add object at 0x11badd590>\n",
      "<keras.layers.core.Activation object at 0x11bcceb50>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11bc41690>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11bc41d10>\n",
      "<keras.layers.core.Activation object at 0x11bd3c390>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11bd8add0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11bd2ac50>\n",
      "<keras.layers.core.Activation object at 0x11bd7a350>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11bef37d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11be66450>\n",
      "<keras.layers.merge.Add object at 0x11bee3fd0>\n",
      "<keras.layers.core.Activation object at 0x11c04f990>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11c06f990>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11bfbd610>\n",
      "<keras.layers.core.Activation object at 0x11c155fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11c144dd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11c1b5e90>\n",
      "<keras.layers.core.Activation object at 0x11c27c290>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11c1c4350>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11c30add0>\n",
      "<keras.layers.merge.Add object at 0x11c3e3d90>\n",
      "<keras.layers.core.Activation object at 0x11c464e50>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11c4eec10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11c426f90>\n",
      "<keras.layers.core.Activation object at 0x11c51bfd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11cccdbd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11c560410>\n",
      "<keras.layers.core.Activation object at 0x11ccbfe90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11d1a7890>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11d197390>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11d118510>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11da816d0>\n",
      "<keras.layers.merge.Add object at 0x11eb38850>\n",
      "<keras.layers.core.Activation object at 0x11eb6a750>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11ebf9b50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11ecce850>\n",
      "<keras.layers.core.Activation object at 0x11ecec910>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11ecffdd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11ed5ef10>\n",
      "<keras.layers.core.Activation object at 0x11ee262d0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11ef40f50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11eeb5fd0>\n",
      "<keras.layers.merge.Add object at 0x11ef8fdd0>\n",
      "<keras.layers.core.Activation object at 0x11f04efd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11f0d9c50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11efc8990>\n",
      "<keras.layers.core.Activation object at 0x11f104f50>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11f197c90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11f14a450>\n",
      "<keras.layers.core.Activation object at 0x11f18aed0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11f3008d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11f270550>\n",
      "<keras.layers.merge.Add object at 0x11f2f1e90>\n",
      "<keras.layers.core.Activation object at 0x11f559d90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11f3c9710>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11f3c96d0>\n",
      "<keras.layers.core.Activation object at 0x11fa5f890>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11faeff90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11fa7eed0>\n",
      "<keras.layers.core.Activation object at 0x11fb03990>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11fcd7b50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11fc47690>\n",
      "<keras.layers.merge.Add object at 0x11fd20790>\n",
      "<keras.layers.core.Activation object at 0x11fda0f10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11fe2cd10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11fd31f90>\n",
      "<keras.layers.core.Activation object at 0x11fe57d90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x11fefc7d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11fe9c2d0>\n",
      "<keras.layers.core.Activation object at 0x11feebfd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x120050990>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x11ffc1610>\n",
      "<keras.layers.merge.Add object at 0x120021c50>\n",
      "<keras.layers.core.Activation object at 0x12015de90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x1201acd10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x12011b6d0>\n",
      "<keras.layers.core.Activation object at 0x120281950>\n",
      "<keras.layers.convolutional.Conv2D object at 0x120313c10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x120313dd0>\n",
      "<keras.layers.core.Activation object at 0x1203ebd90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x12046be50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x12042ef90>\n",
      "<keras.layers.merge.Add object at 0x120543f50>\n",
      "<keras.layers.core.Activation object at 0x1205a5d90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x1205e65d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x120556c10>\n",
      "<keras.layers.core.Activation object at 0x120595610>\n",
      "<keras.layers.convolutional.Conv2D object at 0x12071d890>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x1206bc390>\n",
      "<keras.layers.core.Activation object at 0x12070d390>\n",
      "<keras.layers.convolutional.Conv2D object at 0x1207e56d0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x120834cd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x1207e5690>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x120982fd0>\n",
      "<keras.layers.merge.Add object at 0x120a37f90>\n",
      "<keras.layers.core.Activation object at 0x120ad8ad0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x120b27390>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x120bfda10>\n",
      "<keras.layers.core.Activation object at 0x123e51f90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x123f3ff50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x123eb1fd0>\n",
      "<keras.layers.core.Activation object at 0x123f8add0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x123fbf990>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x123fbf110>\n",
      "<keras.layers.merge.Add object at 0x123fddb50>\n",
      "<keras.layers.core.Activation object at 0x125143c90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x12585ae90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x1250f4c50>\n",
      "<keras.layers.core.Activation object at 0x125135f90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x129cd28d0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x12589e3d0>\n",
      "<keras.layers.core.Activation object at 0x129cc3e90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x129e29d90>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x129d9b710>\n",
      "<keras.layers.merge.Add object at 0x129f02310>\n",
      "<keras.layers.core.Activation object at 0x129f83610>\n",
      "<keras.layers.pooling.AveragePooling2D object at 0x129f54890>\n",
      "<keras.layers.core.Flatten object at 0x129f33c50>\n",
      "<keras.layers.core.Dense object at 0x129fe9fd0>\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')\n",
    "for layer in ResNet50_model.layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151))\n",
    "\n",
    "def dog_detect_per(files):\n",
    "    count = 0;\n",
    "    for file in files:\n",
    "        if dog_detector(file):\n",
    "            count += 1\n",
    "    return count;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "human_detection_per = 0, dog_detection_per = 0\n",
    "for file in human_files_short:\n",
    "    if dog_detector(file):\n",
    "        human_detection_per += 1\n",
    "        \n",
    "for file in dog_files_short:\n",
    "    if dog_detector(file):\n",
    "        dog_detection_per += 1\n",
    "\n",
    "print(\"Dog face detected  in human_files_short are %d \" % human_detection_per)\n",
    "print(\"Dog face detected in dog files are %d \" % dog_detection_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [00:56<00:00, 117.71it/s]\n",
      "  0%|          | 3/835 [00:00<00:42, 19.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading training tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 835/835 [00:06<00:00, 133.77it/s]\n",
      "  3%|▎         | 22/836 [00:00<00:03, 208.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading validation tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 836/836 [00:06<00:00, 133.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading test tensors\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "print(\"Done loading training tensors\")\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "print(\"Done loading validation tensors\")\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "print(\"Done loading test tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following will help to in image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "def get_augmentated_set(training_data, labels, batch_size):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "    return datagen.flow(training_data, labels, batch_size=batch_size)\n",
    "    \n",
    "\n",
    "train_set = get_augmentated_set(train_tensors, train_targets, batch_size=64)\n",
    "\n",
    "valid_set = get_augmentated_set(valid_tensors, valid_targets, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"train_datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_44 (Conv2D)           (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 109, 109, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_11  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 133)               26733     \n",
      "=================================================================\n",
      "Total params: 149,973\n",
      "Trainable params: 149,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Base Model\n",
    "#epoc = 15, accuracy = 9%\n",
    "#checkpoint = 'saved_models/weights.best.from_scratch.hdf5'\n",
    "\"\"\"model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='valid', activation='relu', ))\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='valid', activation='relu', ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu', ))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu', ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(550, activation='relu',))\n",
    "\n",
    "model.add(Dense(133, activation='softmax',))\n",
    "model.summary()\"\"\"\n",
    "\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "\n",
    "\n",
    "\"\"\"model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=5, padding='valid', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=64, kernel_size=5, padding='valid', activation='relu'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='valid', activation='relu' ))\n",
    "model.add(Conv2D(filters=256, kernel_size=3, padding='valid', activation='relu' ))\n",
    "model.add(Dropout(.2))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(600, activation='relu',))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(256, activation='relu',))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "\n",
    "model.summary()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', input_shape=(224, 224, 3), \n",
    "                 kernel_initializer='random_uniform' )) \n",
    "model.add(Conv2D(filters=16, kernel_size=5, padding='same', activation='relu', kernel_initializer='random_uniform')) \n",
    "model.add(Dropout(.2))\n",
    "model.add(MaxPooling2D(pool_size=2)) \n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')) \n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(MaxPooling2D(pool_size=2)) \n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu' )) \n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same',activation='relu' ))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=2)) \n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu' )) \n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu' )) \n",
    "model.add(Dropout(.2))\n",
    "model.add(MaxPooling2D(pool_size=2)) \n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='random_uniform')) \n",
    "model.add(Dropout(.4)) \n",
    "model.add(Dense(133, activation='softmax'))\n",
    "#TODO: Define your architecture.\n",
    "model.summary()\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=3,  activation='relu', ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, activation='relu', ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=3, activation='relu', ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(200, activation='relu',))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Dense(133, activation='softmax',))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3456 - acc: 0.1764\n",
      "Epoch 00001: val_loss improved from inf to 3.51413, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "32/32 [==============================] - 49s 2s/step - loss: 3.3430 - acc: 0.1777 - val_loss: 3.5141 - val_acc: 0.1397\n",
      "Epoch 2/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.2673 - acc: 0.1784\n",
      "Epoch 00002: val_loss did not improve\n",
      "32/32 [==============================] - 49s 2s/step - loss: 3.2554 - acc: 0.1846 - val_loss: 3.7612 - val_acc: 0.1337\n",
      "Epoch 3/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.4124 - acc: 0.1663\n",
      "Epoch 00003: val_loss did not improve\n",
      "32/32 [==============================] - 50s 2s/step - loss: 3.4008 - acc: 0.1680 - val_loss: 3.5601 - val_acc: 0.1518\n",
      "Epoch 4/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3739 - acc: 0.1643\n",
      "Epoch 00004: val_loss did not improve\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.3798 - acc: 0.1641 - val_loss: 3.7283 - val_acc: 0.1266\n",
      "Epoch 5/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3753 - acc: 0.1804\n",
      "Epoch 00005: val_loss did not improve\n",
      "32/32 [==============================] - 50s 2s/step - loss: 3.3801 - acc: 0.1797 - val_loss: 3.6476 - val_acc: 0.1387\n",
      "Epoch 6/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3904 - acc: 0.1754\n",
      "Epoch 00006: val_loss improved from 3.51413 to 3.37087, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "32/32 [==============================] - 49s 2s/step - loss: 3.3991 - acc: 0.1748 - val_loss: 3.3709 - val_acc: 0.1594\n",
      "Epoch 7/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3916 - acc: 0.1724\n",
      "Epoch 00007: val_loss did not improve\n",
      "32/32 [==============================] - 53s 2s/step - loss: 3.4030 - acc: 0.1689 - val_loss: 3.5919 - val_acc: 0.1216\n",
      "Epoch 8/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3786 - acc: 0.1804\n",
      "Epoch 00008: val_loss did not improve\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.3855 - acc: 0.1807 - val_loss: 3.3858 - val_acc: 0.1688\n",
      "Epoch 9/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.2882 - acc: 0.1825\n",
      "Epoch 00009: val_loss did not improve\n",
      "32/32 [==============================] - 49s 2s/step - loss: 3.2939 - acc: 0.1807 - val_loss: 3.6072 - val_acc: 0.1296\n",
      "Epoch 10/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3224 - acc: 0.1784\n",
      "Epoch 00010: val_loss did not improve\n",
      "32/32 [==============================] - 50s 2s/step - loss: 3.3215 - acc: 0.1768 - val_loss: 3.4516 - val_acc: 0.1588\n",
      "Epoch 11/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.2642 - acc: 0.1835\n",
      "Epoch 00011: val_loss did not improve\n",
      "32/32 [==============================] - 53s 2s/step - loss: 3.2743 - acc: 0.1816 - val_loss: 3.6019 - val_acc: 0.1408\n",
      "Epoch 12/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3645 - acc: 0.1946\n",
      "Epoch 00012: val_loss improved from 3.37087 to 3.33992, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "32/32 [==============================] - 49s 2s/step - loss: 3.3587 - acc: 0.1953 - val_loss: 3.3399 - val_acc: 0.1719\n",
      "Epoch 13/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.2986 - acc: 0.1697\n",
      "Epoch 00013: val_loss did not improve\n",
      "32/32 [==============================] - 48s 2s/step - loss: 3.2970 - acc: 0.1673 - val_loss: 3.4090 - val_acc: 0.1719\n",
      "Epoch 14/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3209 - acc: 0.1818\n",
      "Epoch 00014: val_loss did not improve\n",
      "32/32 [==============================] - 50s 2s/step - loss: 3.3162 - acc: 0.1790 - val_loss: 3.3619 - val_acc: 0.1749\n",
      "Epoch 15/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.2689 - acc: 0.1925\n",
      "Epoch 00015: val_loss did not improve\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.2779 - acc: 0.1924 - val_loss: 3.4547 - val_acc: 0.1608\n",
      "Epoch 16/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3750 - acc: 0.1865\n",
      "Epoch 00016: val_loss did not improve\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.3657 - acc: 0.1855 - val_loss: 3.3612 - val_acc: 0.1588\n",
      "Epoch 17/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3329 - acc: 0.2006\n",
      "Epoch 00017: val_loss did not improve\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.3371 - acc: 0.1992 - val_loss: 3.3669 - val_acc: 0.1739\n",
      "Epoch 18/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.2890 - acc: 0.1946\n",
      "Epoch 00018: val_loss did not improve\n",
      "32/32 [==============================] - 52s 2s/step - loss: 3.2770 - acc: 0.1953 - val_loss: 3.5140 - val_acc: 0.1518\n",
      "Epoch 19/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3032 - acc: 0.1764\n",
      "Epoch 00019: val_loss did not improve\n",
      "32/32 [==============================] - 51s 2s/step - loss: 3.2999 - acc: 0.1777 - val_loss: 3.4260 - val_acc: 0.1548\n",
      "Epoch 20/20\n",
      "31/32 [============================>.] - ETA: 1s - loss: 3.3375 - acc: 0.1949\n",
      "Epoch 00020: val_loss did not improve\n",
      "32/32 [==============================] - 49s 2s/step - loss: 3.3358 - acc: 0.1917 - val_loss: 3.3971 - val_acc: 0.1518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c79c710>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "#model.fit(train_set, train_targets, \n",
    "#         validation_data=(valid_set, valid_targets),\n",
    "#          epochs=epochs, batch_size=32, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "train_set = get_augmentated_set(train_tensors, train_targets, batch_size=64)\n",
    "valid_set = get_augmentated_set(valid_tensors, valid_targets, batch_size=64)\n",
    "model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n",
    "                    validation_steps=32, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 17.0000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_14  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 320)               164160    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 133)               42693     \n",
      "=================================================================\n",
      "Total params: 206,853\n",
      "Trainable params: 206,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(320, activation='relu'))\n",
    "VGG16_model.add(Dropout(.4))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 5.9906 - acc: 0.1682\n",
      "Epoch 00001: val_loss improved from inf to 2.14406, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 5s 780us/step - loss: 5.9669 - acc: 0.1704 - val_loss: 2.1441 - val_acc: 0.4551\n",
      "Epoch 2/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 2.2193 - acc: 0.4541\n",
      "Epoch 00002: val_loss improved from 2.14406 to 1.34120, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 281us/step - loss: 2.2175 - acc: 0.4545 - val_loss: 1.3412 - val_acc: 0.6132\n",
      "Epoch 3/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.6355 - acc: 0.5733\n",
      "Epoch 00003: val_loss improved from 1.34120 to 1.26936, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 269us/step - loss: 1.6325 - acc: 0.5734 - val_loss: 1.2694 - val_acc: 0.6647\n",
      "Epoch 4/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.3392 - acc: 0.6456\n",
      "Epoch 00004: val_loss improved from 1.26936 to 1.22053, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 275us/step - loss: 1.3404 - acc: 0.6457 - val_loss: 1.2205 - val_acc: 0.6790\n",
      "Epoch 5/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 1.2102 - acc: 0.6758\n",
      "Epoch 00005: val_loss improved from 1.22053 to 1.08091, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s 272us/step - loss: 1.2106 - acc: 0.6757 - val_loss: 1.0809 - val_acc: 0.6994\n",
      "Epoch 6/20\n",
      "6500/6680 [============================>.] - ETA: 0s - loss: 1.0407 - acc: 0.7220\n",
      "Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 264us/step - loss: 1.0368 - acc: 0.7229 - val_loss: 1.0956 - val_acc: 0.7198\n",
      "Epoch 7/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9936 - acc: 0.7374\n",
      "Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 267us/step - loss: 0.9961 - acc: 0.7373 - val_loss: 1.1527 - val_acc: 0.7198\n",
      "Epoch 8/20\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.9263 - acc: 0.7558\n",
      "Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 267us/step - loss: 0.9220 - acc: 0.7563 - val_loss: 1.1312 - val_acc: 0.7377\n",
      "Epoch 9/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.8876 - acc: 0.7779\n",
      "Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 270us/step - loss: 0.8873 - acc: 0.7781 - val_loss: 1.1255 - val_acc: 0.7389\n",
      "Epoch 10/20\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.8210 - acc: 0.7907\n",
      "Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 270us/step - loss: 0.8211 - acc: 0.7916 - val_loss: 1.2006 - val_acc: 0.7365\n",
      "Epoch 11/20\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.7634 - acc: 0.8017\n",
      "Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 272us/step - loss: 0.7636 - acc: 0.8013 - val_loss: 1.2219 - val_acc: 0.7497\n",
      "Epoch 12/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.7182 - acc: 0.8196\n",
      "Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 270us/step - loss: 0.7227 - acc: 0.8187 - val_loss: 1.2358 - val_acc: 0.7437\n",
      "Epoch 13/20\n",
      "6460/6680 [============================>.] - ETA: 0s - loss: 0.7124 - acc: 0.8189\n",
      "Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 265us/step - loss: 0.7099 - acc: 0.8202 - val_loss: 1.2644 - val_acc: 0.7545\n",
      "Epoch 14/20\n",
      "6480/6680 [============================>.] - ETA: 0s - loss: 0.7200 - acc: 0.8278\n",
      "Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 268us/step - loss: 0.7209 - acc: 0.8274 - val_loss: 1.3245 - val_acc: 0.7401\n",
      "Epoch 15/20\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.6595 - acc: 0.8377\n",
      "Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 264us/step - loss: 0.6619 - acc: 0.8371 - val_loss: 1.3066 - val_acc: 0.7521\n",
      "Epoch 16/20\n",
      "6460/6680 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.8458\n",
      "Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 269us/step - loss: 0.6374 - acc: 0.8466 - val_loss: 1.3786 - val_acc: 0.7557\n",
      "Epoch 17/20\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.6530 - acc: 0.8494\n",
      "Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 265us/step - loss: 0.6511 - acc: 0.8496 - val_loss: 1.3636 - val_acc: 0.7653\n",
      "Epoch 18/20\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.8580\n",
      "Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 264us/step - loss: 0.6341 - acc: 0.8581 - val_loss: 1.4814 - val_acc: 0.7617\n",
      "Epoch 19/20\n",
      "6520/6680 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.8544\n",
      "Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 265us/step - loss: 0.6457 - acc: 0.8540 - val_loss: 1.4571 - val_acc: 0.7557\n",
      "Epoch 20/20\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.6339 - acc: 0.8553\n",
      "Epoch 00020: val_loss did not improve\n",
      "6680/6680 [==============================] - 2s 265us/step - loss: 0.6341 - acc: 0.8560 - val_loss: 1.4342 - val_acc: 0.7533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e634e10>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "#VGG16_model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n",
    "#                    validation_steps=32, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 70.0000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<keras.engine.topology.InputLayer object at 0x1d5a11890>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x1d5a119d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x14462f910>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x1445e5ed0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x13e122a50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x1d5a11e90>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x13d31f510>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x165887a10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x16e2e5910>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x13db24690>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x1d5a1cf10>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x13d4abf90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x13d48b8d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x16d301b10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x16d33cd10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x13d4f2c90>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x16d356950>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x16e354310>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x16e362590>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x16d4a6e10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x1d49b4e50>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x1d4946d10>, False)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 7, 7, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_26  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 20,355,269\n",
      "Trainable params: 330,885\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      " 5/32 [===>..........................] - ETA: 17:58 - loss: 5.0289 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-0c051d333dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n\u001b[0;32m---> 47\u001b[0;31m                     validation_steps=32, callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "def load_pretrained_model(model):\n",
    "    feature_file = \"bottleneck_features/Dog%sData.npz\" % model;\n",
    "    bottleneck_features = np.load(feature_file)\n",
    "    train_model = bottleneck_features['train']\n",
    "    valid_model = bottleneck_features['valid']\n",
    "    test_model = bottleneck_features['test']\n",
    "    return train_model, valid_model, test_model\n",
    "\n",
    "\n",
    "from keras.applications import VGG19\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    " \n",
    "# Create the model\n",
    "model = Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu',  kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n",
    "                    validation_steps=32, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<keras.engine.topology.InputLayer object at 0x234ae7b90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x234ac3dd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x234ac3b90>, False)\n",
      "(<keras.layers.core.Activation object at 0x234b0cf50>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x2431f7fd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x243213c90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2551dffd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x234bd9f90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2270dfd90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25f6be710>, False)\n",
      "(<keras.layers.core.Activation object at 0x2271b6310>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x227236610>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22730c4d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2271e6bd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x227352950>, False)\n",
      "(<keras.layers.merge.Add object at 0x22739da10>, False)\n",
      "(<keras.layers.core.Activation object at 0x2274f5b90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2274a8d90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2274a8f90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2275a9b90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2275dbe10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2275dbe50>, False)\n",
      "(<keras.layers.core.Activation object at 0x22761df10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x227718110>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x227718150>, False)\n",
      "(<keras.layers.merge.Add object at 0x22777af10>, False)\n",
      "(<keras.layers.core.Activation object at 0x2278eddd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2278be950>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22788c950>, False)\n",
      "(<keras.layers.core.Activation object at 0x2279c7350>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x227a54ed0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2279f6990>, False)\n",
      "(<keras.layers.core.Activation object at 0x227b2d350>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x227b70ed0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x227b5ef50>, False)\n",
      "(<keras.layers.merge.Add object at 0x227c82510>, False)\n",
      "(<keras.layers.core.Activation object at 0x227d16e90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x227d44e50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x227cc8990>, False)\n",
      "(<keras.layers.core.Activation object at 0x227ddd290>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x227dc9e10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x227dc9b90>, False)\n",
      "(<keras.layers.core.Activation object at 0x227e41a90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x227fa8110>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2283a9310>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x227f56e50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22839b310>, False)\n",
      "(<keras.layers.merge.Add object at 0x22841df50>, False)\n",
      "(<keras.layers.core.Activation object at 0x228534cd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x228571dd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2285623d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x22e3e2510>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22e472fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22e4129d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x22e549390>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22e577810>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22e579890>, False)\n",
      "(<keras.layers.merge.Add object at 0x22e6a1550>, False)\n",
      "(<keras.layers.core.Activation object at 0x22e6d2dd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22e733f10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22e763e90>, False)\n",
      "(<keras.layers.core.Activation object at 0x22e7fd2d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22e81ff10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22e81fdd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x22e860ed0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22e9c30d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22e973810>, False)\n",
      "(<keras.layers.merge.Add object at 0x22f7fc350>, False)\n",
      "(<keras.layers.core.Activation object at 0x22f86ec90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22f83e410>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22f7ef350>, False)\n",
      "(<keras.layers.core.Activation object at 0x22f85eed0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22f9d6ed0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22f988f50>, False)\n",
      "(<keras.layers.core.Activation object at 0x22fab2450>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22fb2f190>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22fae1a90>, False)\n",
      "(<keras.layers.merge.Add object at 0x22fc09490>, False)\n",
      "(<keras.layers.core.Activation object at 0x22fc283d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22fc95f90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22fcc8e10>, False)\n",
      "(<keras.layers.core.Activation object at 0x22fd61390>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22fda5b90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22fd4cc90>, False)\n",
      "(<keras.layers.core.Activation object at 0x22fdc2dd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22ff39dd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22ff2e350>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22fed78d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2300113d0>, False)\n",
      "(<keras.layers.merge.Add object at 0x230ebb5d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x230eeedd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x230f2bed0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x230f1ce90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2310326d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2310935d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x231044fd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x23116e490>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x231ed7dd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x231f2d1d0>, False)\n",
      "(<keras.layers.merge.Add object at 0x2320034d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2320486d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x232092fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2320b6d90>, False)\n",
      "(<keras.layers.core.Activation object at 0x23215d710>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2321a2bd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2321cfe50>, False)\n",
      "(<keras.layers.core.Activation object at 0x2322caf90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x232338e10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2322ba290>, False)\n",
      "(<keras.layers.merge.Add object at 0x23232a390>, False)\n",
      "(<keras.layers.core.Activation object at 0x23249e810>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2324c0fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x232410410>, False)\n",
      "(<keras.layers.core.Activation object at 0x23248f3d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2325bced0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2325a8c90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2326d2550>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x232703dd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x232765f10>, False)\n",
      "(<keras.layers.merge.Add object at 0x2328292d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2328bbfd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2328cce50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23287f3d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x232990ed0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2329c5c90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2329f40d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x232b7b350>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x234e9cc90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x232b6c350>, False)\n",
      "(<keras.layers.merge.Add object at 0x234fc2350>, False)\n",
      "(<keras.layers.core.Activation object at 0x235048350>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2372a8f50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x235036c10>, False)\n",
      "(<keras.layers.core.Activation object at 0x237490a90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23762be90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2374e1f10>, False)\n",
      "(<keras.layers.core.Activation object at 0x2377457d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2377a8f90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2377a8610>, False)\n",
      "(<keras.layers.merge.Add object at 0x237829a90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2379e0110>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23798fe50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23798f7d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x237d999d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2380599d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x237ffa890>, False)\n",
      "(<keras.layers.core.Activation object at 0x238029c90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23842f8d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2388c7410>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2383e1890>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2388f9d10>, False)\n",
      "(<keras.layers.merge.Add object at 0x238b03410>, False)\n",
      "(<keras.layers.core.Activation object at 0x238ba1e90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x238e15950>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x238b90dd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x238493250>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x240a4efd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x234c549d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2409a4110>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2552bee90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x238f75950>, False)\n",
      "(<keras.layers.merge.Add object at 0x239045590>, False)\n",
      "(<keras.layers.core.Activation object at 0x239074ed0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2390d1fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2390fbed0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2391cf310>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23920fb10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2391bec10>, False)\n",
      "(<keras.layers.core.Activation object at 0x23922cf50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x239753d50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2396f7850>, False)\n",
      "(<keras.layers.merge.Add object at 0x23983c390>, False)\n",
      "(<keras.layers.core.Activation object at 0x2398add10>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x23987c450>, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 1, 1, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_28  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 24,705,029\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      " 7/32 [=====>........................] - ETA: 5:21 - loss: 5.8350 - acc: 0.0112 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-89c3ee00f558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n\u001b[0;32m---> 45\u001b[0;31m                     validation_steps=32, callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###Resnet\n",
    "\n",
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "\n",
    "#Inception\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "#Load the VGG model\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in resnet.layers[:]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in resnet.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    " \n",
    "# Create the model\n",
    "model = Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(resnet)\n",
    " \n",
    "# Add new layers\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu',  kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.ResNet-50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n",
    "                    validation_steps=32, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 53s 1us/step\n",
      "87924736/87910968 [==============================] - 53s 1us/step\n",
      "(<keras.engine.topology.InputLayer object at 0x16770e610>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22995af50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x22996cc50>, False)\n",
      "(<keras.layers.core.Activation object at 0x22998da90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23829aed0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23c006d10>, False)\n",
      "(<keras.layers.core.Activation object at 0x24781ef50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x226cb8b10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x226cb8a90>, False)\n",
      "(<keras.layers.core.Activation object at 0x226d08d50>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x227071fd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x22709fc50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x227091250>, False)\n",
      "(<keras.layers.core.Activation object at 0x2270626d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23befae10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23be9dc50>, False)\n",
      "(<keras.layers.core.Activation object at 0x23becbad0>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x23c28cb50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23ef7b490>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23f419bd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x23f425d50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23c6446d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23f526050>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23c668e90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23f578910>, False)\n",
      "(<keras.layers.core.Activation object at 0x23eee7f50>, False)\n",
      "(<keras.layers.core.Activation object at 0x23f54b8d0>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x240b73a50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23c2aae50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23ef8aad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x240706ed0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x23f667490>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23c2aafd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23c67ad90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x23f679c90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x240bc6e90>, False)\n",
      "(<keras.layers.core.Activation object at 0x23c27d350>, False)\n",
      "(<keras.layers.core.Activation object at 0x23f3f6fd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x240b51f90>, False)\n",
      "(<keras.layers.core.Activation object at 0x240c70510>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x240cb2190>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2429a6690>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2429a6450>, False)\n",
      "(<keras.layers.core.Activation object at 0x2429e8e10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x240e11cd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x242b57e50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x240d72e50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x242b15cd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x241710050>, False)\n",
      "(<keras.layers.core.Activation object at 0x242be0950>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x242d78450>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x240cb2990>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24174cbd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x242c427d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x242c13050>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x240c90c90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x240de2f10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x242c25a50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24415ed50>, False)\n",
      "(<keras.layers.core.Activation object at 0x240dd4e90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2429837d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x242d57990>, False)\n",
      "(<keras.layers.core.Activation object at 0x242d6a810>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x2441d6bd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24480be10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24480bbd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x244832350>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x244308dd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x244bef9d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2442f91d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x244bdb790>, False)\n",
      "(<keras.layers.core.Activation object at 0x244316dd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x244e9fa50>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x245264a10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x244226c50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x244455ad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x244f03cd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x244ed44d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2441b9910>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2443f4c50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x244ee7b90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2452b7ed0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2442b4510>, False)\n",
      "(<keras.layers.core.Activation object at 0x244448890>, False)\n",
      "(<keras.layers.core.Activation object at 0x244f46e10>, False)\n",
      "(<keras.layers.core.Activation object at 0x245396810>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x246dd5690>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x246f04e90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x246f33b10>, False)\n",
      "(<keras.layers.core.Activation object at 0x24701f290>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x247041b10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x246ef3cd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x247173910>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x246e33f10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24715d050>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x246dc6a10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24718f3d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x246edfcd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x24719cdd0>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x24729cb10>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x2472db810>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2477be850>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24779f890>, False)\n",
      "(<keras.layers.core.Activation object at 0x247951ad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x247972a90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x247790410>, False)\n",
      "(<keras.layers.core.Activation object at 0x247a5d3d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x247415dd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x247a80fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2473f7250>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x247a80d50>, False)\n",
      "(<keras.layers.core.Activation object at 0x247405290>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a08c190>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x247513190>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a112250>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x247513d90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a0ff890>, False)\n",
      "(<keras.layers.core.Activation object at 0x247566d90>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a0aba90>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x24a339b10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2472eea50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x247654810>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a22ab10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a328a50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x247345f10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x247676d50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a1fca50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a3289d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2472ceed0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2476c5410>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a317290>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a434590>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x24a456b90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a9616d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a961490>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a982590>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24aaf0ad0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24aade810>, False)\n",
      "(<keras.layers.core.Activation object at 0x24aab1f10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a5d6fd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24abfdd50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a5b6fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24abe09d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a627e90>, False)\n",
      "(<keras.layers.core.Activation object at 0x24acfb210>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a7137d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24ad1ca90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a6f4810>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24ad2bc10>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a825a50>, False)\n",
      "(<keras.layers.core.Activation object at 0x24ae4ec90>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x24afa1f10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a469e90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a8348d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24ae7bd50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24afce9d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a469910>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24a862bd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24ad0e9d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24afbb790>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a496c50>, False)\n",
      "(<keras.layers.core.Activation object at 0x24a93f810>, False)\n",
      "(<keras.layers.core.Activation object at 0x24ae6ce90>, False)\n",
      "(<keras.layers.core.Activation object at 0x24afad3d0>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x24b0ea1d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b5e2c90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b5c4910>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b623ad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b715a10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b704250>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b731a90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b21a810>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b852290>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b237a90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b846c50>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b247910>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b85d8d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b337210>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b9b1910>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b337c10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b983f50>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b388d90>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b9904d0>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x24bbee750>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b0ea5d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24b4f8f10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24bace510>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24bbddad0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b0dddd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24b4b1d50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24baafe10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24bbdd350>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b1d9150>, False)\n",
      "(<keras.layers.core.Activation object at 0x24b57ea10>, False)\n",
      "(<keras.layers.core.Activation object at 0x24bb1fe50>, False)\n",
      "(<keras.layers.core.Activation object at 0x24bce9610>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x24bd0ba90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2419ddb10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x241ad43d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x241a2d610>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2381b3b50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2381c3bd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x23bf94150>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x250f2edd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x251482c90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x250f4dfd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251466910>, False)\n",
      "(<keras.layers.core.Activation object at 0x250f9de90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2514c0ad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2510877d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2515a6a10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251068810>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251596250>, False)\n",
      "(<keras.layers.core.Activation object at 0x25119ba50>, False)\n",
      "(<keras.layers.core.Activation object at 0x2515c2a90>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x251826910>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x24a478490>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2511ab8d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2516d9290>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2517fbe50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x24bd2e510>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2511d8bd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2516ccc50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2518074d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x250ee6750>, False)\n",
      "(<keras.layers.core.Activation object at 0x23d1e6210>, False)\n",
      "(<keras.layers.core.Activation object at 0x2516e38d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2517e7e50>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x25195be50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x251b85d50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251b85150>, False)\n",
      "(<keras.layers.core.Activation object at 0x251bd7d50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x251cc6fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251ce8d10>, False)\n",
      "(<keras.layers.core.Activation object at 0x251d383d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25192f410>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x251e31810>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25192fa10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251e13850>, False)\n",
      "(<keras.layers.core.Activation object at 0x251a5c610>, False)\n",
      "(<keras.layers.core.Activation object at 0x251f42a90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x251a6e750>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x251f53910>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251a89d90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x251f62090>, False)\n",
      "(<keras.layers.core.Activation object at 0x251a7c250>, False)\n",
      "(<keras.layers.core.Activation object at 0x252052390>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x252072190>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x2520a41d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2525828d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25259dbd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x25268b610>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2521eca90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2525b0cd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25223ffd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2526d0610>, False)\n",
      "(<keras.layers.core.Activation object at 0x2522ecb50>, False)\n",
      "(<keras.layers.core.Activation object at 0x2526ddc10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2521ffad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25245d310>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x252841250>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25295cb10>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x25291e5d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x252096dd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25231cbd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x252453c50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x252830890>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25292da50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x252a5ba50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2520d4950>, False)\n",
      "(<keras.layers.core.Activation object at 0x25240a4d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x25246cd10>, False)\n",
      "(<keras.layers.core.Activation object at 0x2527dea90>, False)\n",
      "(<keras.layers.core.Activation object at 0x252a48290>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x252b67450>, False)\n",
      "(<keras.layers.core.Activation object at 0x252179f10>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x252582fd0>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x252a6bb10>, False)\n",
      "(<keras.layers.core.Activation object at 0x252ac9cd0>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x252b88f10>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2530a4b50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x253093310>, False)\n",
      "(<keras.layers.core.Activation object at 0x253199690>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x252cfc090>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x253200a90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x252d09250>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2531e0550>, False)\n",
      "(<keras.layers.core.Activation object at 0x252e04950>, False)\n",
      "(<keras.layers.core.Activation object at 0x25324ffd0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x252e698d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x252f83f50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x253211ad0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x253473310>, False)\n",
      "(<keras.layers.pooling.AveragePooling2D object at 0x2535a18d0>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x252b88e50>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x252e39f10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x252f668d0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x253331bd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x253464c50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x2535ba750>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x252bd6c10>, False)\n",
      "(<keras.layers.core.Activation object at 0x252e46490>, False)\n",
      "(<keras.layers.core.Activation object at 0x252fd4d50>, False)\n",
      "(<keras.layers.core.Activation object at 0x25341b4d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x25347ed10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2536ab610>, False)\n",
      "(<keras.layers.core.Activation object at 0x252cc5650>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x2530a4710>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x2535a1fd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2535d0cd0>, False)\n",
      "(<keras.layers.merge.Concatenate object at 0x2536de310>, False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-40be32be76ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Add the vgg convolutional base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Add new layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m   2076\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensor_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2078\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2079\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mrun_internal_graph\u001b[0;34m(self, inputs, masks)\u001b[0m\n\u001b[1;32m   2227\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m                                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2229\u001b[0;31m                             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2230\u001b[0m                             output_masks = _to_list(layer.compute_mask(computed_tensor,\n\u001b[1;32m   2231\u001b[0m                                                                        computed_mask))\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/layers/normalization.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    186\u001b[0m                          K.moving_average_update(self.moving_variance,\n\u001b[1;32m    187\u001b[0m                                                  \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                                                  self.momentum)],\n\u001b[0m\u001b[1;32m    189\u001b[0m                         inputs)\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mmoving_average_update\u001b[0;34m(x, value, momentum)\u001b[0m\n\u001b[1;32m    999\u001b[0m     \"\"\"\n\u001b[1;32m   1000\u001b[0m     return moving_averages.assign_moving_average(\n\u001b[0;32m-> 1001\u001b[0;31m         x, value, momentum, zero_debias=True)\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.pyc\u001b[0m in \u001b[0;36massign_moving_average\u001b[0;34m(variable, value, decay, zero_debias, name)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mzero_debias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mupdate_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_zero_debias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mupdate_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.pyc\u001b[0m in \u001b[0;36m_zero_debias\u001b[0;34m(unbiased_var, value, decay)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       biased_var = variable_scope.get_variable(\n\u001b[0;32m--> 207\u001b[0;31m           \u001b[0m_maybe_get_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"biased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbiased_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m           trainable=False)\n\u001b[1;32m    209\u001b[0m       local_step = variable_scope.get_variable(\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.pyc\u001b[0m in \u001b[0;36m_maybe_get_unique\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    197\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         vs_vars = [x.op.name for x in\n\u001b[0;32m--> 199\u001b[0;31m                    variable_scope.get_variable_scope().global_variables()]\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mfull_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfull_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvs_vars\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mglobal_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;34m\"\"\"Get this scope's global variables.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlocal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[0;34m\"\"\"Get this scope's variables.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0mscope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(key, scope)\u001b[0m\n\u001b[1;32m   4850\u001b[0m     \u001b[0mcollected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4851\u001b[0m   \"\"\"\n\u001b[0;32m-> 4852\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(self, name, scope)\u001b[0m\n\u001b[1;32m   3374\u001b[0m         \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3376\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3377\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0;34m\"\"\"The name of this variable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation was not named: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "\n",
    "#Inception\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "#Load the VGG model\n",
    "inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in inception.layers[:]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in inception.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    " \n",
    "# Create the model\n",
    "model = Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(inception)\n",
    " \n",
    "# Add new layers\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu',  kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Inception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n",
    "                    validation_steps=32, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 62s 1us/step\n",
      "83697664/83683744 [==============================] - 62s 1us/step\n",
      "(<keras.engine.topology.InputLayer object at 0x253822810>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25e62d290>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25f1e9b90>, False)\n",
      "(<keras.layers.core.Activation object at 0x25e798090>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25e83f990>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25e798350>, False)\n",
      "(<keras.layers.core.Activation object at 0x25e7dccd0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x25e90ef10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25ea79610>, False)\n",
      "(<keras.layers.core.Activation object at 0x25ea6d510>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x25eb50f90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25ebf7990>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25e8fdcd0>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x25ec1b610>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25e94c750>, False)\n",
      "(<keras.layers.merge.Add object at 0x25ecee8d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x25ee4a650>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x25eeccdd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25ee7c6d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x25ee29710>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x25f0c2f10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25fbb4610>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25ed81fd0>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x25f0b1dd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25ed50ed0>, False)\n",
      "(<keras.layers.merge.Add object at 0x25fbd6a10>, False)\n",
      "(<keras.layers.core.Activation object at 0x25fdebe10>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x25e95c950>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25ff65790>, False)\n",
      "(<keras.layers.core.Activation object at 0x25fea2cd0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x25fff8fd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x26000bf90>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x25fd24990>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x25ffe7910>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x25fc08f10>, False)\n",
      "(<keras.layers.merge.Add object at 0x260101c10>, False)\n",
      "(<keras.layers.core.Activation object at 0x260162710>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x260162d90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x260177a50>, False)\n",
      "(<keras.layers.core.Activation object at 0x26022fad0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x260270810>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x260323d10>, False)\n",
      "(<keras.layers.core.Activation object at 0x260306b90>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2603e0950>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x260422b90>, False)\n",
      "(<keras.layers.merge.Add object at 0x2604755d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2605bce90>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x26057f790>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2605aae90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2606975d0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2606dc590>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2606fcdd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x26073ff90>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x26086df10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2608aced0>, False)\n",
      "(<keras.layers.merge.Add object at 0x26088bc90>, False)\n",
      "(<keras.layers.core.Activation object at 0x260a19e50>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2609c8e10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2609fbcd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x260ae6c90>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x260b4c550>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x260bc7f50>, False)\n",
      "(<keras.layers.core.Activation object at 0x260ba8450>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x260c86650>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x260cf6910>, False)\n",
      "(<keras.layers.merge.Add object at 0x260c52390>, False)\n",
      "(<keras.layers.core.Activation object at 0x260ea3d50>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x260dc0090>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x260e54e50>, False)\n",
      "(<keras.layers.core.Activation object at 0x260e63a50>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x260fcefd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x260f5c490>, False)\n",
      "(<keras.layers.core.Activation object at 0x260fbd910>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2610ddc90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261130510>, False)\n",
      "(<keras.layers.merge.Add object at 0x261152e90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2612bdfd0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x26128cd90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261315690>, False)\n",
      "(<keras.layers.core.Activation object at 0x261387a50>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2613cdcd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x26140ab90>, False)\n",
      "(<keras.layers.core.Activation object at 0x2613a5d90>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x261586c10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261556210>, False)\n",
      "(<keras.layers.merge.Add object at 0x261579fd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x261661290>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x261661690>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x2616e6e90>, False)\n",
      "(<keras.layers.core.Activation object at 0x261815510>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2618a6dd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261806a90>, False)\n",
      "(<keras.layers.core.Activation object at 0x261973d90>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x261973810>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261a25ed0>, False)\n",
      "(<keras.layers.merge.Add object at 0x261a08090>, False)\n",
      "(<keras.layers.core.Activation object at 0x261ae3b10>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x261ae3490>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261c2e590>, False)\n",
      "(<keras.layers.core.Activation object at 0x261c5fed0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x261c5fc10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261ca38d0>, False)\n",
      "(<keras.layers.core.Activation object at 0x261dd0cd0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x261dd0990>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261e13910>, False)\n",
      "(<keras.layers.merge.Add object at 0x261f0e910>, False)\n",
      "(<keras.layers.core.Activation object at 0x261f72810>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x261fb3cd0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x261f3eed0>, False)\n",
      "(<keras.layers.core.Activation object at 0x2620c6c50>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2620c6310>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x26210fad0>, False)\n",
      "(<keras.layers.core.Activation object at 0x26223ef90>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2621d6d90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x26227fb10>, False)\n",
      "(<keras.layers.merge.Add object at 0x262359cd0>, False)\n",
      "(<keras.layers.core.Activation object at 0x26248cc50>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x2624b5ed0>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x26254df10>, False)\n",
      "(<keras.layers.core.Activation object at 0x26255d090>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x26263cd90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x262602a50>, False)\n",
      "(<keras.layers.convolutional.Conv2D object at 0x260155510>, False)\n",
      "(<keras.layers.pooling.MaxPooling2D object at 0x2626dfc90>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x26238f090>, False)\n",
      "(<keras.layers.merge.Add object at 0x2627ab410>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x262776b10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x262909850>, False)\n",
      "(<keras.layers.core.Activation object at 0x2628401d0>, False)\n",
      "(<keras.layers.convolutional.SeparableConv2D object at 0x26293ff10>, False)\n",
      "(<keras.layers.normalization.BatchNormalization object at 0x26299af10>, False)\n",
      "(<keras.layers.core.Activation object at 0x2629f3f90>, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Model)             (None, 7, 7, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_29  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 21,978,797\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "25/32 [======================>.......] - ETA: 1:34 - loss: 4.4255 - acc: 0.1460"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-7cb2a50a8e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n\u001b[0;32m---> 39\u001b[0;31m                     validation_steps=32, callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###Resnet\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "#Load the VGG model\n",
    "xception = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in xception.layers[:]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in xception.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    " \n",
    "# Create the model\n",
    "model = Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(xception)\n",
    " \n",
    "# Add new layers\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu',  kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit_generator(train_set, steps_per_epoch=32, epochs=epochs, validation_data=valid_set, verbose=1,\n",
    "                    validation_steps=32, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Define your architecture.\n",
    "def load_architecture(train):\n",
    "    model = Sequential()\n",
    "    model.add(GlobalAveragePooling2D(input_shape=train.shape[1:]))\n",
    "    #for layer in base_model.layers:\n",
    "    model.add(Dense(300, activation='relu', kernel_initializer='RandomUniform'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(133, activation='softmax'))\n",
    "    model.summary()\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    for layer in model.layers:\n",
    "        print(layer)\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "\n",
    "def model_test(model_name):\n",
    "    train_set, valid_set, test_set = load_pretrained_model(model_name)\n",
    "    model = load_architecture(train_set)\n",
    "    model = model_compile(model)\n",
    "\n",
    "    file_path = \"saved_models/weights.best.%s.pretrained.hdf5\" % ModelName\n",
    "    checkpointer = ModelCheckpoint(filepath= file_path, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    model.fit(train_set, train_targets, \n",
    "              validation_data=(valid_set, valid_targets),\n",
    "              epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    model_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in test_set]\n",
    "\n",
    "    # report test accuracy\n",
    "    test_accuracy = 100*np.sum(np.array(model_predictions)==np.argmax(test_targets, axis=1))/len(model_predictions)\n",
    "    print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_31  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 300)               614700    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 133)               40033     \n",
      "=================================================================\n",
      "Total params: 654,733\n",
      "Trainable params: 654,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x25e77bc10>\n",
      "<keras.layers.core.Dense object at 0x165d69bd0>\n",
      "<keras.layers.core.Dropout object at 0x167cb5e90>\n",
      "<keras.layers.core.Dense object at 0x25d62dc90>\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 3.0728 - acc: 0.2992\n",
      "Epoch 00001: val_loss improved from inf to 1.16884, saving model to saved_models/weights.best.Resnet50.pretrained.hdf5\n",
      "6680/6680 [==============================] - 13s 2ms/step - loss: 3.0625 - acc: 0.3012 - val_loss: 1.1688 - val_acc: 0.6886\n",
      "Epoch 2/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 1.3621 - acc: 0.6116\n",
      "Epoch 00002: val_loss improved from 1.16884 to 0.77312, saving model to saved_models/weights.best.Resnet50.pretrained.hdf5\n",
      "6680/6680 [==============================] - 4s 618us/step - loss: 1.3614 - acc: 0.6117 - val_loss: 0.7731 - val_acc: 0.7617\n",
      "Epoch 3/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 1.0004 - acc: 0.7050\n",
      "Epoch 00003: val_loss improved from 0.77312 to 0.75278, saving model to saved_models/weights.best.Resnet50.pretrained.hdf5\n",
      "6680/6680 [==============================] - 4s 639us/step - loss: 1.0014 - acc: 0.7045 - val_loss: 0.7528 - val_acc: 0.7641\n",
      "Epoch 4/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.8323 - acc: 0.7483\n",
      "Epoch 00004: val_loss improved from 0.75278 to 0.65749, saving model to saved_models/weights.best.Resnet50.pretrained.hdf5\n",
      "6680/6680 [==============================] - 4s 623us/step - loss: 0.8299 - acc: 0.7496 - val_loss: 0.6575 - val_acc: 0.7916\n",
      "Epoch 5/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.7177 - acc: 0.7834\n",
      "Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 621us/step - loss: 0.7200 - acc: 0.7831 - val_loss: 0.7501 - val_acc: 0.7892\n",
      "Epoch 6/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.6416 - acc: 0.8011\n",
      "Epoch 00006: val_loss improved from 0.65749 to 0.65460, saving model to saved_models/weights.best.Resnet50.pretrained.hdf5\n",
      "6680/6680 [==============================] - 4s 576us/step - loss: 0.6417 - acc: 0.8010 - val_loss: 0.6546 - val_acc: 0.8084\n",
      "Epoch 7/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8217\n",
      "Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 543us/step - loss: 0.5718 - acc: 0.8222 - val_loss: 0.7135 - val_acc: 0.8108\n",
      "Epoch 8/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.8412\n",
      "Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 575us/step - loss: 0.5253 - acc: 0.8410 - val_loss: 0.6832 - val_acc: 0.8096\n",
      "Epoch 9/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8441\n",
      "Epoch 00009: val_loss improved from 0.65460 to 0.64903, saving model to saved_models/weights.best.Resnet50.pretrained.hdf5\n",
      "6680/6680 [==============================] - 4s 618us/step - loss: 0.4792 - acc: 0.8439 - val_loss: 0.6490 - val_acc: 0.8192\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8586\n",
      "Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 613us/step - loss: 0.4513 - acc: 0.8590 - val_loss: 0.6621 - val_acc: 0.8192\n",
      "Epoch 11/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8645\n",
      "Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 614us/step - loss: 0.4307 - acc: 0.8648 - val_loss: 0.7531 - val_acc: 0.8132\n",
      "Epoch 12/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8745\n",
      "Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 628us/step - loss: 0.4072 - acc: 0.8744 - val_loss: 0.7322 - val_acc: 0.8228\n",
      "Epoch 13/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3829 - acc: 0.8821\n",
      "Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 612us/step - loss: 0.3853 - acc: 0.8820 - val_loss: 0.6934 - val_acc: 0.8120\n",
      "Epoch 14/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8856\n",
      "Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 618us/step - loss: 0.3550 - acc: 0.8853 - val_loss: 0.7571 - val_acc: 0.8216\n",
      "Epoch 15/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8915\n",
      "Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 621us/step - loss: 0.3510 - acc: 0.8915 - val_loss: 0.7949 - val_acc: 0.8263\n",
      "Epoch 16/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8976\n",
      "Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 624us/step - loss: 0.3517 - acc: 0.8976 - val_loss: 0.7994 - val_acc: 0.8347\n",
      "Epoch 17/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8958\n",
      "Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 630us/step - loss: 0.3547 - acc: 0.8954 - val_loss: 0.7998 - val_acc: 0.8323\n",
      "Epoch 18/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.9032\n",
      "Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 643us/step - loss: 0.3291 - acc: 0.9031 - val_loss: 0.7774 - val_acc: 0.8395\n",
      "Epoch 19/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9079\n",
      "Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 615us/step - loss: 0.3120 - acc: 0.9079 - val_loss: 0.8211 - val_acc: 0.8263\n",
      "Epoch 20/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9124\n",
      "Epoch 00020: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s 613us/step - loss: 0.2930 - acc: 0.9121 - val_loss: 0.7976 - val_acc: 0.8359\n",
      "Test accuracy: 82.0000%\n"
     ]
    }
   ],
   "source": [
    "model_test(\"ResNet50\")\n",
    "model_test(\"VGG16\")\n",
    "model_test(\"VGG19\")\n",
    "model_test(\"Inception\")\n",
    "model_test(\"Xception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6620/6680 [============================>.] - ETA: 0s - loss: 2.9790 - acc: 0.3134\n",
      "Epoch 00001: val_loss improved from inf to 1.17236, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 5s 730us/step - loss: 2.9659 - acc: 0.3151 - val_loss: 1.1724 - val_acc: 0.6707\n",
      "Epoch 2/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 1.3264 - acc: 0.6144\n",
      "Epoch 00002: val_loss improved from 1.17236 to 0.84774, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s 406us/step - loss: 1.3266 - acc: 0.6138 - val_loss: 0.8477 - val_acc: 0.7174\n",
      "Epoch 3/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.9843 - acc: 0.7083\n",
      "Epoch 00003: val_loss improved from 0.84774 to 0.74315, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s 403us/step - loss: 0.9827 - acc: 0.7085 - val_loss: 0.7432 - val_acc: 0.7545\n",
      "Epoch 4/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.8338 - acc: 0.7530\n",
      "Epoch 00004: val_loss improved from 0.74315 to 0.69794, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s 407us/step - loss: 0.8353 - acc: 0.7524 - val_loss: 0.6979 - val_acc: 0.7952\n",
      "Epoch 5/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.7288 - acc: 0.7777\n",
      "Epoch 00005: val_loss improved from 0.69794 to 0.66995, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s 412us/step - loss: 0.7316 - acc: 0.7769 - val_loss: 0.6699 - val_acc: 0.7940\n",
      "Epoch 6/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.6105 - acc: 0.8125\n",
      "Epoch 00006: val_loss improved from 0.66995 to 0.65252, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "6680/6680 [==============================] - 3s 396us/step - loss: 0.6125 - acc: 0.8117 - val_loss: 0.6525 - val_acc: 0.8048\n",
      "Epoch 7/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.5690 - acc: 0.8243\n",
      "Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 390us/step - loss: 0.5663 - acc: 0.8250 - val_loss: 0.6883 - val_acc: 0.7976\n",
      "Epoch 8/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8366\n",
      "Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 406us/step - loss: 0.5172 - acc: 0.8364 - val_loss: 0.6650 - val_acc: 0.8120\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8479\n",
      "Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 401us/step - loss: 0.4764 - acc: 0.8485 - val_loss: 0.7861 - val_acc: 0.7976\n",
      "Epoch 10/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.8609\n",
      "Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 404us/step - loss: 0.4461 - acc: 0.8621 - val_loss: 0.7247 - val_acc: 0.8120\n",
      "Epoch 11/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8703\n",
      "Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 403us/step - loss: 0.4194 - acc: 0.8705 - val_loss: 0.7151 - val_acc: 0.8168\n",
      "Epoch 12/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8774\n",
      "Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 406us/step - loss: 0.3916 - acc: 0.8774 - val_loss: 0.7626 - val_acc: 0.8036\n",
      "Epoch 13/20\n",
      "6640/6680 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.8798\n",
      "Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 409us/step - loss: 0.3835 - acc: 0.8801 - val_loss: 0.8110 - val_acc: 0.8287\n",
      "Epoch 14/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8909\n",
      "Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 401us/step - loss: 0.3608 - acc: 0.8907 - val_loss: 0.7225 - val_acc: 0.8240\n",
      "Epoch 15/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8941\n",
      "Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 399us/step - loss: 0.3597 - acc: 0.8942 - val_loss: 0.7081 - val_acc: 0.8323\n",
      "Epoch 16/20\n",
      "6660/6680 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.9011\n",
      "Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 428us/step - loss: 0.3323 - acc: 0.9012 - val_loss: 0.8012 - val_acc: 0.8371\n",
      "Epoch 17/20\n",
      "6560/6680 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9081\n",
      "Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 423us/step - loss: 0.3048 - acc: 0.9075 - val_loss: 0.7943 - val_acc: 0.8132\n",
      "Epoch 18/20\n",
      "6580/6680 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9061\n",
      "Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 423us/step - loss: 0.3120 - acc: 0.9060 - val_loss: 0.7779 - val_acc: 0.8251\n",
      "Epoch 19/20\n",
      "6540/6680 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9125\n",
      "Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 409us/step - loss: 0.2880 - acc: 0.9124 - val_loss: 0.8137 - val_acc: 0.8299\n",
      "Epoch 20/20\n",
      "6600/6680 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.9124\n",
      "Epoch 00020: val_loss did not improve\n",
      "6680/6680 [==============================] - 3s 403us/step - loss: 0.3099 - acc: 0.9120 - val_loss: 0.8139 - val_acc: 0.8240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13ebd4ed0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG19_model.fit(train_VGG19, train_targets, \n",
    "          validation_data=(valid_VGG19, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "VGG19_model.load_weights('saved_models/weights.best.VGG19.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "VGG19_model.load_weights('saved_models/weights.best.VGG19.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 82.0000%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate c\n",
    "# get index of predicted dog breed for each image in test set\n",
    "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "best_model = VGG19_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "def predict_breed(image_path):\n",
    "    tensor_image = path_to_tensor(image_path)\n",
    "    \n",
    "    prediction = best_model.predict(extract_VGG19(tensor_image))\n",
    "    \n",
    "    \n",
    "    # sort predicted breeds by highest probability, extract the top N predictions\n",
    "    breeds_predicted = [dog_names[idx] for idx in np.argsort(prediction)[::-1][:5]]\n",
    "    confidence_predicted = np.sort(prediction)[::-1][:top_N]\n",
    "\n",
    "    # take prediction, lookup in dog_names, return value\n",
    "    return breeds_predicted, confidence_predicted\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_prediction(path):\n",
    "    breeds, confidence = predict_breed(path)\n",
    "    img = mpimg.imread(path)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # since the dog detector worked better, and we don't have \n",
    "    # access to softmax probabilities from dog and face detectors\n",
    "    # we'll first check for dog detection, and only if there are no dogs\n",
    "    # detected we'll check for humans\n",
    "    if dog_detector(path):\n",
    "        imgplot = plt.imshow(img)\n",
    "        print('its a Dog, and its breed is {}.'.format(breeds[0].replace(\"_\", \" \")))\n",
    "        \n",
    "        print('\\n\\nTop 4 predictions )')\n",
    "        for i, j in zip(breeds, confidence):\n",
    "            print('Predicted breed: {} with a confidence of {:.4f}'.format(i.replace(\"_\", \" \"), j))\n",
    "        \n",
    "    elif face_detector(path):\n",
    "\n",
    "        imgplot = plt.imshow(img)\n",
    "        print('It is a Human, and most resemble dog is {}.'.format(breeds[0].replace(\"_\", \" \")))\n",
    "    else:\n",
    "        raise ValueError('Could not detect dogs or humans in image.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected global_average_pooling2d_23_input to have shape (1, 1, 2048) but got array with shape (7, 7, 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-b34acec2455d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## at least 6 images on your computer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Feel free to use as many code cells as needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/d1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-1a7ff98f6b5d>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbreeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_breed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-3d52255bd5fb>\u001b[0m in \u001b[0;36mpredict_breed\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtensor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_VGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1027\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1781\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shafidayatar/anaconda/envs/python2.7-DA/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected global_average_pooling2d_23_input to have shape (1, 1, 2048) but got array with shape (7, 7, 512)"
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "make_prediction('images/d1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
